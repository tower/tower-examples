# Develop with DeepSeek R1 on local GPUs, deploy with serverless inference

In this example we will develop a Tower app that uses the Deepseek R1 model to analyze a GitHub issue and its comments and proposes the best course of action to address the issue. While developing and debugging, we will use the local GPUs of the dev machine for inference. Once we move the app to a production environment, we will use a serverless inference provider. 
 
This example demonstrates the "local dev, cloud prod" capabilities of Tower:
- Use ollama to host a local inference server for a smaller 14B version of the Deepseek R1 model
- Use the local GPUs (e.g. Apple Silicon) to save on inference costs during development and avoid inference rate throttling
- Use Tower's --local mode to run the app on your dev machine  
- Once you are happy with your app, move the app to Tower cloud, and use Together.AI as the inference provider
- To maintain flexibility with inference providers, use HuggingFace Hub as the router of inference calls



## Tower Actions Framework

In this example we also introduce a simple framework of typical data+ai app actions (read data inputs, transform them, run inference, write data outputs) for Python.

## Install and Set Up dependencies

### Install ollama and a small Deepseek R1 model

During development we will use [`ollama`](https://ollama.com/) to serve as a local inference server.

```
pip install ollama
ollama pull deepseek-r1:14b
```

### Sign Up for Hugging Face  

In production, we will use Hugging Face Hub to route our inference calls. You don't have to use the Hub and you can call inference providers directly, but using the Hub is free, and it adds flexibility to being able to switch providers that provide you better value, e.g. have lower latency, higher request rates, lower costs, better availability etc.

You should [sign up for Hugging Face](https://huggingface.co/join) and get the Hugging Face [access token](https://huggingface.co/docs/hub/en/security-tokens). Note this token as you will need it later. 

### Install Hugging Face Hub

```
pip install huggingface_hub>=0.29.0
```

### Sign Up for Together.ai

In this example we use Together.ai as our serverless inference provider for the full Deepseek R1 model. Sign up for [together.ai](https://www.together.ai/) and note its [access key](https://docs.together.ai/reference/authentication-1). 

### Enable Together.ai in Hugging Face Hub

Follow this [quickstart](https://docs.together.ai/docs/quickstart-using-hugging-face-inference) to enable Together.ai in the Hugging Face Hub. You will enter your Together.ai access token in the Hugging Face Hub settings. Once you do that, you can use your Hugging Face access token to make inference calls from your Tower app. 

## Create the app in Tower

The app name should match what's in the Towerfile in this directory.

```bash
tower apps create --name=deepseek-summarize
```

## Deploy the app to Tower

```bash
tower deploy
```

## Creating app secrets

Secrets in Tower are environment variables that will automatically be passed to your app.

When running in local mode, no secrets are needed for this app.

When running in the "prod" environment, you will need to define a secret for the Hugging Face token to run the app.

```bash
tower secrets create --environment="prod" \
  --name=HF_TOKEN --value='<your Hugging Face token>'
```

## Running the app

You can run the app using the Tower CLI. You don't need to specify the app name; the tower CLI will figure out what app to run based on the Towerfile.

### Running locally

When running the app locally, a local `ollama` server will serve the inference requests.

Start ollama in a separate terminal window.

```bash
ollama run deepseek-r1:14b
```

Then run the app in --local mode, use the following command. The command has some default values for the inputs (the GitHub issue) and output locations. 

```bash
tower run --local \
  --parameter=gh_repo_owner='dlt-hub' \
  --parameter=gh_repo='dlt' \
  --parameter=gh_issue_number=933 \
  --parameter=out_last_response_bucket_url='./out/gh_summary_last_response' \
  --parameter=out_full_chat_bucket_url='./out/gh_summary_full_chat' \
  --parameter=model_to_use='deepseek-r1:14b'
```

Here is what each parameter means:

- gh_repo_owner : The owner of the GitHub repo
- gh_repo : The name of the GitHub repo
- gh_issue_number : The number of the issue in that repo
- out_last_response_bucket_url : path to a folder containing the file with the summary generated by the model
- out_full_chat_bucket_url : path to a folder containing the file with the GitHub thread and the model-generated summary
- model_to_use : version of the model to use to generate the summary


### Running in "prod"

To run on Tower cloud, first make sure that you created a secret for the Hugging Face token. See "Creating app secrets" section.

Then, run the app with a slightly different set of parameters and by replacing the --local setting with the "prod" environment setting.

```bash
tower run --environment="prod" \
  --parameter=gh_repo_owner='dlt-hub' \
  --parameter=gh_repo='dlt' \
  --parameter=gh_issue_number=933 \
  --parameter=out_last_response_bucket_url='./out/gh_summary_last_response' \
  --parameter=out_full_chat_bucket_url='./out/gh_summary_full_chat' \
  --parameter=model_to_use='deepseek-ai/DeepSeek-R1' \
  --parameter=inference_provider='together' \
  --parameter=max_tokens=1000
```

Two new parameters are relevant for "prod" runs: 
- inference_provider : The inference provider hosting the DeepSeek model, in our case 'together'
- max_tokens : The maximum length of output, measured in tokens


## Check the run status

You can use the following command to see how the app is progressing. Again, no
need to supply an app name as long as you're in a directory with a Towerfile.

```bash
$ tower apps show
```

You can also use the Tower [web UI](https://app.tower.dev) to learn more about the status of your app run.


## Troubleshooting

### ModuleNotFoundError error

If you get an error when executing one of the examples
```
ModuleNotFoundError: No module named 'core'
```
just add to the Python Path
```
export PYTHONPATH=.:$PYTHONPATH
```

### NotImplementedError on MPS device error

If you are trying to do local Pytorch inference on Apple Silicon and get an error when executing one of the examples
```
NotImplementedError: The operator 'aten::isin.Tensor_Tensor_out' is not currently implemented for the MPS device.
```
just run this before running the example script
```
export PYTORCH_ENABLE_MPS_FALLBACK=1
```

