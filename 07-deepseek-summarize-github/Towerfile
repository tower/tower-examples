[app]
name = "deepseek-summarize"
script = "./summarize.py"

source = [
	"./.dlt/config.toml",
	"./core/**/*.py",
	"./requirements.txt",
    "./summarize.py"
]

[[parameters]]
name = "gh_repo_owner"
description = "The owner of the GitHub repo"
default = "tower"

[[parameters]]
name = "gh_repo"
description = "The name of the GitHub repo"
default = "tower-cli"

[[parameters]]
name = "gh_issue_number"
description = "The issue number in that repo"
default = "11"

[[parameters]]
name = "out_last_response_bucket_url"
description = "Path to an output folder to store a file with the options generated by the model"
default = "./out/gh_summary_last_response"

[[parameters]]
name = "out_full_chat_bucket_url"
description = "Path to an output folder to store a file with the entire GitHub thread + options generated by the model"
default = "./out/gh_summary_full_chat"

[[parameters]]
name = "model_to_use"
description = "Name and version of the model to use to generate the summary. E.g. deepseek-r1:14b when using ollama or deepseek-ai/DeepSeek-R1 when using Together.ai"
default = "deepseek-r1:14b"

[[parameters]]
name = "inference_provider"
description = "The inference provider hosting the DeepSeek model, e.g. 'together'"
default = "together"

[[parameters]]
name = "max_tokens"
description = "The maximum length of output, measured in tokens"
default = "1000"

